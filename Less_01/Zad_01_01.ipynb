{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Осуществим предобработку данных с Твиттера, чтобы отчищенный данные в дальнейшем использовать для задачи классификации. Данный датасет содержит негативные (label = 1) и нейтральные (label = 0) высказывания.\n",
        "Для работы объединим train_df и test_df.\n",
        "\n",
        "Задания:\n",
        "\n",
        "1) Заменим html-сущности (к примеру: &lt; &gt; &amp;). \"&lt;\" заменим на “<” и \"&amp;\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape(). Всю предобработку делаем в новом столбце 'clean_tweet'\n",
        "\n",
        "2) Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
        " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
        " - для для замены @user на пробел, необходимо использовать re.sub()\n",
        "при применении функции необходимо использовать np.vectorize(function).\n",
        "\n",
        "3) Изменим регистр твитов на нижний с помощью .lower().\n",
        "\n",
        "4) Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова).\n",
        "\n",
        "5) Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
        "\n",
        "6) Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
        "\n",
        "7) Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
        "\n",
        "8) Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
        "\n",
        "9) Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
        "\n",
        "10) Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n",
        "\n",
        "11) Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'.\n",
        "\n",
        "12) Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
        "\n",
        "13) Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
        "\n",
        "14) Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
        "\n",
        "15) Сохраним результат предобработки в pickle-файл."
      ],
      "metadata": {
        "id": "AnzgHjXtLDPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тема “Предобработка текста с помощью Python”\n",
        "Осуществим предобработку данных с Твиттера, чтобы очищенные данные в дальнейшем\n",
        "использовать для задачи классификации. Данный датасет содержит негативные (label = 1)\n",
        "и нейтральные (label = 0) высказывания. Для работы объединим train_df и test_df.\n",
        "Задания:\n",
        "1. Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим\n",
        "функцию:\n",
        "  *  для того, чтобы найти все вхождения паттерна в тексте, необходимо\n",
        "использовать re.findall(pattern, input_txt)\n",
        "  *  для для замены @user на пробел, необходимо использовать re.sub()\n",
        "2. Изменим регистр твитов на нижний с помощью .lower().\n",
        "3. Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя\n",
        "apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в\n",
        "тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в\n",
        "качестве ключа (сокращенного слова), то заменить ключ на значение (полную\n",
        "версию слова).\n",
        "4. Заменим сокращения на их полные формы, используя short_word_dict. Для этого\n",
        "воспользуемся функцией, используемой в предыдущем пункте.\n",
        "5. Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict.\n",
        "Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
        "6. Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
        "7. Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
        "8. Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
        "9. Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if\n",
        "len(w)>1]).\n",
        "10. Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый\n",
        "столбец 'tweet_token'.\n",
        "11. Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. \n",
        "12. Создадим столбец\n",
        "'tweet_token_filtered' без стоп-слов."
      ],
      "metadata": {
        "id": "eRU5_lAaMgyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import casual_tokenize, RegexpTokenizer, TreebankWordTokenizer\n",
        "from nltk.util import ngrams\n",
        "import pandas as pd\n",
        "import math"
      ],
      "metadata": {
        "id": "P6zpfxVVLs0E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import regex as re\n",
        "import html\n",
        "import nltk\n",
        "import pickle\n",
        "from dicts import apostrophe_dict, emoticon_dict, short_word_dict\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn7YE2M0NfgI",
        "outputId": "5a553f75-0bc8-48a1-943c-dabfd7bfdb51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STOP_WORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "STEMMER = nltk.stem.PorterStemmer()\n",
        "LEMMATIZER = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "# Data load\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/NLP/Less_01/train_tweets.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP/Less_01/test_tweets.csv')\n",
        "df = train_df.append(test_df, ignore_index = True, sort = False)"
      ],
      "metadata": {
        "id": "rpb8kOMqNeMn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.info())\n",
        "print(test_df.info())\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8pRWeTkP2Iv",
        "outputId": "5149c301-7ef1-48b2-a6cf-66748128de81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31962 entries, 0 to 31961\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      31962 non-null  int64 \n",
            " 1   label   31962 non-null  int64 \n",
            " 2   tweet   31962 non-null  object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 749.2+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17197 entries, 0 to 17196\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      17197 non-null  int64 \n",
            " 1   tweet   17197 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 268.8+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 49159 entries, 0 to 49158\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   id      49159 non-null  int64  \n",
            " 1   label   31962 non-null  float64\n",
            " 2   tweet   49159 non-null  object \n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 1.1+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_from_dict(text, source_dict):\n",
        "    \n",
        "    \"\"\"Поиск по тексту, сопоставление токенов через dict, размещение по умолчанию, если не в dict\"\"\"\n",
        "    \n",
        "    return \" \".join([source_dict.get(word, word) for word in text.split()])\n",
        "\n",
        "\n",
        "def remove_onechar_tokens(text):\n",
        "    \n",
        "    \"\"\"Поиск по тексту, удаление односимвольных токенов\"\"\"\n",
        "    \n",
        "    return ' '.join([w for w in text.split() if len(w)>1])\n",
        "\n",
        "\n",
        "def filter_stop_words(tokens, stop_words=STOP_WORDS):\n",
        "    \n",
        "    \"\"\"Удалить стоп-слова из токенов\"\"\"\n",
        "    \n",
        "    return [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "\n",
        "def stem_tokens(tokens, stemmer=STEMMER):\n",
        "    \n",
        "    \"\"\"Предварительная обработка стемминга\"\"\"\n",
        "    \n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "\n",
        "def lemmatize_tokens(tokens, lemmatizer=LEMMATIZER):\n",
        "    \n",
        "    \"\"\"Лемматизация\"\"\"\n",
        "    \n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "\n",
        "def preprocess(df,\n",
        "               src_col='tweet',\n",
        "               clean_col='clean_tweet',\n",
        "               token_col='tweet_token',\n",
        "               filter_col='tweet_token_filtered',\n",
        "               stemmed_col='tweet_stemmed',\n",
        "               lem_col='tweet_lemmatized'\n",
        "              ):\n",
        "     \n",
        "        \n",
        "    # 1. Чистый HTML-контекст\n",
        "    df[clean_col] = df[src_col].apply(lambda x: html.unescape(x))\n",
        "    \n",
        "    # 2. Удалить ссылки @user\n",
        "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'@[\\w]*','', x))\n",
        "    \n",
        "    # 3. Правильный регистр в нижнем регистре\n",
        "    df[clean_col] = df[clean_col].str.lower()\n",
        "    \n",
        "    # 4. Изменить апострофы\n",
        "    vfunc = np.vectorize(replace_from_dict)\n",
        "    df[clean_col] = vfunc(df[clean_col], apostrophe_dict)\n",
        "    \n",
        "    # 5. Продлить короткие слова\n",
        "    df[clean_col] = vfunc(df[clean_col], short_word_dict)\n",
        "    \n",
        "    # 6. Заменить смайлики\n",
        "    df[clean_col] = vfunc(df[clean_col], emoticon_dict)\n",
        "    \n",
        "    # 7. Заменить знаки препинания на пробелы\n",
        "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
        "    \n",
        "    # 8. Заменить специальные символы на пробелы\n",
        "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))\n",
        "    \n",
        "    # 9. Заменить цифры на пробелы\n",
        "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n",
        "    \n",
        "    # 10. Отбросьте односимвольное слово\n",
        "    vfunc = np.vectorize(remove_onechar_tokens)\n",
        "    df[clean_col] = vfunc(df[clean_col])\n",
        "    \n",
        "    # 11. Токенизировать текст\n",
        "    df[token_col] = df[clean_col].apply(lambda x:  nltk.tokenize.word_tokenize(x))\n",
        "    \n",
        "    # 12. Отфильтровать стоп-слова\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    df[filter_col] = df[token_col].apply(lambda x: filter_stop_words(x))\n",
        "    \n",
        "    # 13. Apply stemming\n",
        "    df[stemmed_col] = df[filter_col].apply(lambda x: stem_tokens(x))\n",
        "    \n",
        "    # 14. Лемматизировать\n",
        "    df[lem_col] = df[stemmed_col].apply(lambda x: lemmatize_tokens(x))\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "B2rBJlxfp0bM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}